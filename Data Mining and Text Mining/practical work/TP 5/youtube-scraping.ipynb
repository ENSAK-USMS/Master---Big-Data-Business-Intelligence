{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "DEVELOPER_KEY = \"AIzaSyD7Lc3LOf48k1sAGA0GdfDha7OjqoFJYlE\"\n",
    "\n",
    "youtube = googleapiclient.discovery.build(\n",
    "    api_service_name, api_version, developerKey=DEVELOPER_KEY)\n",
    "\n",
    "request = youtube.commentThreads().list(\n",
    "    part=\"snippet\",\n",
    "    videoId=\"xuCn8ux2gbs\",\n",
    "    maxResults=1000\n",
    ")\n",
    "response = request.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available fields for comment snippet:\n",
      "channelId\n",
      "videoId\n",
      "textDisplay\n",
      "textOriginal\n",
      "authorDisplayName\n",
      "authorProfileImageUrl\n",
      "authorChannelUrl\n",
      "authorChannelId\n",
      "canRate\n",
      "viewerRating\n",
      "likeCount\n",
      "publishedAt\n",
      "updatedAt\n"
     ]
    }
   ],
   "source": [
    "snippet_keys = response['items'][0]['snippet']['topLevelComment']['snippet'].keys()\n",
    "print(\"Available fields for comment snippet:\")\n",
    "for key in snippet_keys:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for item in response['items']:\n",
    "    comment = item['snippet']['topLevelComment']['snippet']\n",
    "    username = comment['authorDisplayName']\n",
    "    commentContent = comment['textDisplay']\n",
    "    comment_date = comment['publishedAt']                                                               \n",
    "    likes = comment['likeCount']\n",
    "    data.append({'Username': username, 'Comment': commentContent, 'Comment Date': comment_date,\"likes\":likes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Convert data to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Export DataFrame to Excel\n",
    "df.to_excel('youtube_comments.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abdob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abdob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abdob\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre prosessing on comments\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def clean_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    # Remove stopwords\n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(w) for w in tokens_without_sw])\n",
    "    return text\n",
    "\n",
    "df[\"Comment_cleaned\"]=df['Comment'].apply(clean_text)\n",
    "\n",
    "# Export DataFrame to Excel\n",
    "df.to_excel('youtube_comments_cleaned.xlsx', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text = \"\"\"\n",
    "The innovative technologies emerged with the industrial revolution ‘‘Industry 4.0\" as well as the new ones on\n",
    "the way of advanced digitalization enable delivering enhanced, value-added and cost-effective manufacturing\n",
    "and service operations. One of the first areas of focus for Industry 4.0 applications is operations related to\n",
    "healthcare services. Effective management of healthcare resources, clinical care processes, service planning,\n",
    "delivery and evaluation of healthcare operations are essential for a well-functioning healthcare system. Yet,\n",
    "with the adoption of technologies such as Internet of Health Things, Medical Cyber–Physical Systems, Machine\n",
    "Learning, and Big Data (BD), the healthcare sector has recognized the relevance of Industry 4.0. The concept\n",
    "of BD offered numerous advantages and opportunities in this field. It changed the way information is gathered,\n",
    "shared and utilized. Hence, in this study our main ambition is to provide readers with a review of publications\n",
    "which lie within the intersection of Industry 4.0, BD, and healthcare operations and give future perspectives.\n",
    "Our review shows that BD constitutes an important place on the technologies Industry 4.0 provides in the\n",
    "healthcare domain. It helps design, improve, analyze, assess and optimize operations in the domain.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abdob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abdob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abdob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Username</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Comment Date</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Comment_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@eskimoded9480</td>\n",
       "      <td>Second part?</td>\n",
       "      <td>2024-05-14T13:51:29Z</td>\n",
       "      <td>0</td>\n",
       "      <td>second part</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@hiccup7457</td>\n",
       "      <td>AP world students lock in our exam is tmrw</td>\n",
       "      <td>2024-05-14T12:57:08Z</td>\n",
       "      <td>1</td>\n",
       "      <td>ap world student lock exam tmrw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@funny-guy4426</td>\n",
       "      <td>15:46 no words</td>\n",
       "      <td>2024-05-14T12:35:20Z</td>\n",
       "      <td>0</td>\n",
       "      <td>word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Astrologist-uq5zf</td>\n",
       "      <td>I saw the n word</td>\n",
       "      <td>2024-05-14T11:44:42Z</td>\n",
       "      <td>0</td>\n",
       "      <td>saw n word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@reenavox</td>\n",
       "      <td>will i ever remember this? no. am i watching t...</td>\n",
       "      <td>2024-05-14T08:54:59Z</td>\n",
       "      <td>0</td>\n",
       "      <td>ever remember watching bc silly voice goofy wo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Username                                            Comment  \\\n",
       "0      @eskimoded9480                                       Second part?   \n",
       "1         @hiccup7457         AP world students lock in our exam is tmrw   \n",
       "2      @funny-guy4426                                     15:46 no words   \n",
       "3  @Astrologist-uq5zf                                   I saw the n word   \n",
       "4           @reenavox  will i ever remember this? no. am i watching t...   \n",
       "\n",
       "           Comment Date  Likes  \\\n",
       "0  2024-05-14T13:51:29Z      0   \n",
       "1  2024-05-14T12:57:08Z      1   \n",
       "2  2024-05-14T12:35:20Z      0   \n",
       "3  2024-05-14T11:44:42Z      0   \n",
       "4  2024-05-14T08:54:59Z      0   \n",
       "\n",
       "                                     Comment_cleaned  \n",
       "0                                        second part  \n",
       "1                    ap world student lock exam tmrw  \n",
       "2                                               word  \n",
       "3                                         saw n word  \n",
       "4  ever remember watching bc silly voice goofy wo...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Set up YouTube API\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "DEVELOPER_KEY = \"AIzaSyD7Lc3LOf48k1sAGA0GdfDha7OjqoFJYlE\"  # Replace with your actual API key\n",
    "\n",
    "youtube = googleapiclient.discovery.build(\n",
    "    api_service_name, api_version, developerKey=DEVELOPER_KEY)\n",
    "\n",
    "# Retrieve comments from YouTube video\n",
    "video_id = \"xuCn8ux2gbs\"\n",
    "request = youtube.commentThreads().list(\n",
    "    part=\"snippet\",\n",
    "    videoId=video_id,\n",
    "    maxResults=100,\n",
    "    textFormat=\"plainText\"  # Return comments in plain text format\n",
    ")\n",
    "response = request.execute()\n",
    "\n",
    "# Extract data from response\n",
    "data = []\n",
    "for item in response['items']:\n",
    "    comment = item['snippet']['topLevelComment']['snippet']\n",
    "    username = comment['authorDisplayName']\n",
    "    comment_content = comment['textDisplay']\n",
    "    comment_date = comment['publishedAt']\n",
    "    likes = comment['likeCount']\n",
    "    data.append({\n",
    "        'Username': username,\n",
    "        'Comment': comment_content,\n",
    "        'Comment Date': comment_date,\n",
    "        'Likes': likes\n",
    "    })\n",
    "\n",
    "# Convert data to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Export DataFrame to Excel\n",
    "df.to_excel('youtube_comments.xlsx', index=False)\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words('english')]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(w) for w in tokens_without_sw])\n",
    "    return text\n",
    "\n",
    "# Apply cleaning function to comments\n",
    "df[\"Comment_cleaned\"] = df['Comment'].apply(clean_text)\n",
    "\n",
    "# Export cleaned DataFrame to Excel\n",
    "df.to_excel('youtube_comments_cleaned.xlsx', index=False)\n",
    "\n",
    "\n",
    "\n",
    "df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
